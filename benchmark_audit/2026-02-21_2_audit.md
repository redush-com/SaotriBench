# SaotriBench Audit Report

**Date**: 2026-02-21
**Benchmark status**: IN PROGRESS (~68% coverage, run still active)
**Data snapshot**: 115 best results (1 per model+task pair)
**Benchmark version**: Current main branch

---

## 1. Executive Summary

SaotriBench evaluates LLM agents on **hidden requirement discovery** — the ability to infer undisclosed constraints from structured test feedback and iteratively refine solutions across phases. This audit analyzes an in-progress benchmark run of 14 models across 12 tasks (168 possible pairs, 115 completed so far).

**Key findings:**
- Only **18/115 (16%)** runs end in full completion — most models struggle with the core SAOTRI challenge of discovering hidden rules from feedback
- **task_03_validate_brackets** stands out at 92% completion — a well-calibrated reference task
- Several easy tasks (task_01, task_02) have 0% completion across all 14 models, raising questions about feedback signal clarity
- Infrastructure issues (14-way parallelism) introduce bias: 18 errors and 13 timeouts are network-related, not model failures
- Token efficiency strongly correlates with comprehension: successful models use 2-5x fewer tokens

---

## 2. Benchmark Coverage Status

### 2.1 Overall Statistics

| Metric | Value |
|--------|-------|
| Models tested | 14 |
| Tasks available | 12 |
| Maximum pairs | 168 |
| Completed pairs | 115 (68%) |
| Full coverage (all 12 tasks) | 4 models: Llama 3.3 70B, Gemini 3.1 Pro, Claude Sonnet 4.6 |
| Partial coverage (5-7 tasks) | 8 models (benchmark still running) |
| Minimal coverage (< 5 tasks) | 2 models: Trinity Large, Kimi K2.5 |

### 2.2 Run Outcome Distribution

| Status | Count | Share | Meaning |
|--------|-------|-------|---------|
| completed | 18 | 16% | All phases passed |
| failed | 66 | 57% | Max attempts exhausted on a phase |
| error | 18 | 16% | Infrastructure/API crash |
| timeout | 13 | 11% | Response time exceeded limit |

**Note**: `error` and `timeout` are partially infrastructure artifacts from running 14 models in parallel against OpenRouter. These should be discounted when evaluating model capability.

---

## 3. Model Performance Analysis

### 3.1 Rankings by Task Completion

| Model | Tier | Tasks | Completed | Rate | Phases | Phase Rate | Errors | Timeouts |
|-------|------|-------|-----------|------|--------|------------|--------|----------|
| Claude Opus 4.6 | strong | 7 | 3 | 43% | 23/37 | 62% | 0 | 1 |
| GPT-5.2 Codex | strong | 7 | 2 | 29% | 18/37 | 49% | 1 | 1 |
| Claude Sonnet 4.6 | strong | 11 | 2 | 18% | 29/83 | 35% | 2 | 1 |
| DeepSeek V3.2 | strong | 11 | 2 | 18% | 28/83 | 34% | 2 | 1 |
| Gemini 3.1 Pro | strong | 12 | 2 | 17% | 26/95 | 27% | 5 | 1 |
| GLM 5 | strong | 6 | 1 | 17% | 14/33 | 42% | 0 | 1 |
| MiniMax M2.5 | strong | 6 | 1 | 17% | 14/33 | 42% | 0 | 0 |
| Grok 4.1 Fast | strong | 7 | 1 | 14% | 16/37 | 43% | 1 | 0 |
| Kimi K2.5 | strong | 5 | 1 | 20% | 12/25 | 48% | 1 | 1 |
| Llama 3.3 70B | weak | 12 | 1 | 8% | 20/95 | 21% | 1 | 2 |
| Trinity Large | strong | 5 | 0 | 0% | 7/28 | 25% | 0 | 0 |

### 3.2 Model Observations

**Top performer: Claude Opus 4.6** — Only model to complete task_04_sort_objects (6-phase multi-key sorting). Demonstrates strong hidden-requirement discovery and context retention across phases. 62% phase completion rate is the highest.

**Strong feedback learners** (completed task_00_fizzbuzz — discovered "Bazz" rule for divisible-by-7 from test violations): Claude Opus 4.6, Claude Sonnet 4.6, DeepSeek V3.2, GPT-5.2 Codex, Gemini 3.1 Pro.

**Weak feedback learners** (stuck at phase 1 on fizzbuzz despite seeing `divisible_by_7` violation): Llama 3.3 70B, MiniMax M2.5, Trinity Large, Grok 4.1 Fast. These models receive the violation signal but fail to translate it into a code fix — a core SAOTRI capability gap.

**Token-inefficient models**: Grok 4.1 Fast consumes 80K-100K tokens per task with no completions. This confirms SaotriBench's finding that "brute-force approaches consume 2-3x more tokens with worse results."

**Infrastructure-impacted models**: Gemini 3.1 Pro (5 errors), Kimi K2.5 (612s timeout on fizzbuzz with only 980 tokens). Results for these models are unreliable and should be re-run with lower parallelism.

### 3.3 SAOTRI Capability Assessment

Per the SAOTRI framework dimensions:

| Capability | Strong Models | Weak Models |
|------------|--------------|-------------|
| **Long-context retention** | Opus 4.6 (retains rules across 6 phases in sort_objects) | All weak-tier models lose context after 2-3 phases |
| **Iterative refinement** | GPT-5.2 (3 attempts on fizzbuzz), DeepSeek V3.2 | Grok 4.1 Fast (6 attempts, no improvement) |

---

## 4. Task Analysis

### 4.1 Completion Heatmap

```
Task                          Diff    Phases  Completed/Tested  Rate
────────────────────────────────────────────────────────────────────
task_03_validate_brackets     medium  5       12/13             92%
task_00_fizzbuzz              easy    4        5/14             36%
task_04_sort_objects          medium  6        1/14              7%
task_01_transform_list        easy    3        0/14              0%
task_02_merge_dicts           easy    4        0/10              0%
task_05_text_processor        medium  7        0/14              0%
task_06_cache_eviction        medium  8        0/13              0%
task_07_expression_parser     medium  9        0/5               0%  *
task_08_access_control        medium  10       0/5               0%  *
task_09_schedule_optimizer    hard    12       0/5               0%  *
task_10_data_pipeline         hard    12       0/3               0%  *
task_11_version_resolver      hard    15       0/5               0%  *

* Incomplete coverage — benchmark still running
```

### 4.2 Phase Blocking Analysis

Where models get stuck — the phase at which most models fail:

| Task | Blocking Phase | Violation | Models Stuck | Analysis |
|------|---------------|-----------|-------------|----------|
| task_00_fizzbuzz | 1 | `divisible_by_7` (30x) | 9/14 | Models fail to infer new divisor rule from violation feedback |
| task_01_transform_list | 1-2 | `negative_handling` (59x), `cap_overflow` (27x) | 14/14 | No model discovers abs() behavior or capping threshold |
| task_02_merge_dicts | 1 | `numeric_conflict` (21x), `string_conflict` (22x) | 6/10 | Type-aware merge semantics not inferred from feedback |
| task_04_sort_objects | 2-3 | `missing_key` (50x), `descending` (26x) | 13/14 | Missing-key placement and "-" prefix convention not discovered |
| task_05_text_processor | 1-2 | `unicode_combining` (47x), `quoted_content` (38x) | 14/14 | Unicode NFC and quote-aware whitespace are domain-specific |
| task_06_cache_eviction | 2 | `ttl_expiry` (54x), `capacity_eviction` (19x) | 11/13 | TTL mechanics not reverse-engineered from violations |

### 4.3 Detailed Task Assessment

#### task_03_validate_brackets — REFERENCE TASK (92% completion)

This task demonstrates ideal SaotriBench calibration:
- Clear phase progression: basic matching → nesting → error positions → custom brackets → mixed
- Violation feedback is actionable — agents can map `error_position` violations to specific code changes
- 12/13 models complete all 5 phases; only  (weakest model) fails at phase 3
- **Benchmark insight**: This proves the evaluation framework works — when feedback signals are clear, most models can discover hidden requirements

#### task_00_fizzbuzz — EFFECTIVE DISCRIMINATOR (36% completion)

Splits models into two clear groups:
- **Discoverers** (5 models): Read `divisible_by_7` violation → infer "Bazz" rule → discover combination ordering → handle edge cases
- **Non-discoverers** (9 models): See the violation but regenerate the same code, failing to translate feedback into understanding

This is exactly what SaotriBench is designed to measure. The 36% completion rate is appropriate for an "easy" task that tests basic feedback literacy.

#### task_01_transform_list — POTENTIAL CALIBRATION ISSUE (0% completion)

**0/14 models complete this "easy" task.** Phase analysis:
- Phase 0: All models pass (basic doubling)
- Phase 1: 7 models stuck — `negative_handling` requires discovering that negatives need `abs()` before transformation
- Phase 2: 7 models stuck — `cap_overflow` requires discovering the output cap value

**Concern**: If no model — including Claude Opus 4.6 (the top performer) — can discover the abs() requirement from violation feedback alone, the feedback signal for this task may be insufficient. The violation `negative_handling` tells the agent WHAT is wrong but may not provide enough signal for the agent to infer HOW to fix it. Compare with task_00 where `divisible_by_7` directly hints at the solution.

**Question for task designers**: Does the feedback.json for phase 1 include input/output examples that would allow an agent to see `[-3, 2] → expected [6, 4]`? If yes, the agents have a feedback literacy problem. If no, the feedback signal needs enrichment.

#### task_02_merge_dicts — POTENTIAL CALIBRATION ISSUE (0% completion)

**0/10 models tested so far.** Three strong models (Opus 4.6, Sonnet 4.6, Gemini 3.1 Pro) reach phase 3 but fail on `list_merge`. The type-aware conflict resolution in phase 1 (numeric→sum, string→concat) is difficult to reverse-engineer from violation signals alone.

**Same question**: Does the structured feedback provide enough input/output signal for agents to discover merge semantics?

#### task_05_text_processor — HIGH DIFFICULTY (0% completion)

0/14 completion is expected for a task requiring Unicode NFC normalization and quote-aware whitespace. This tests deep domain knowledge discovery. The blocking violations (`unicode_combining` at 47x) suggest models don't know about Unicode normalization forms, and feedback signals alone may not teach this concept.

**Assessment**: This task may be testing domain knowledge rather than feedback literacy. Worth monitoring whether any model discovers NFC normalization from violations when all models complete their runs.

#### task_06_cache_eviction — WALL AT PHASE 2 (0% completion)

All 13 models that reached this task get stuck at phase 2 (`ttl_expiry` — 54 violations). This is the single most common violation across the entire benchmark. The eviction policy, TTL mechanics, and expiry semantics require complex reverse-engineering.

**Assessment**: Phase 2 may represent a steep difficulty cliff. Consider whether the violation feedback provides sufficient signal about TTL behavior (e.g., does it show timestamps, expiry times?).

#### Tasks 07-11 — INSUFFICIENT DATA

Only 3-5 models have reached these tasks. No completions yet. Notable:
- **task_07_expression_parser**: DeepSeek V3.2 reached phase 6/9 — the deepest penetration into any 0%-completion task. Then hit an error (likely network).
- **task_11_version_resolver**: Claude Sonnet 4.6 reached phase 5/15 — showing some models can navigate complex multi-phase tasks.

These need full model coverage before assessment is meaningful.

---

## 5. Violation Hotspots

### 5.1 Top 20 Most Common Violations

| # | Violation | Count | Primary Task | Assessment |
|---|-----------|-------|-------------|------------|
| 1 | `correct_output/negative_handling` | 59 | task_01 | Feedback signal may be insufficient |
| 2 | `correct_output/ttl_expiry` | 54 | task_06 | Complex domain concept |
| 3 | `correct_output/missing_key` | 50 | task_04 | Convention-based, hard to infer |
| 4 | `correct_output/unicode_combining` | 47 | task_05 | Domain knowledge gap |
| 5 | `no_mutation/error` | 43 | task_01, task_02 | Side-effect constraint — invariant (I) |
| 6 | `correct_output/quoted_content` | 38 | task_05 | Complex parsing rule |
| 7 | `correct_output/divisible_by_7` | 30 | task_00 | Feedback works — 5 models discover it |
| 8 | `correct_output/cap_overflow` | 27 | task_01 | Threshold discovery |
| 9 | `correct_output/descending` | 26 | task_04 | Convention-based |
| 10 | `correct_output/string_conflict` | 22 | task_02 | Semantic inference required |
| 11 | `correct_output/numeric_conflict` | 21 | task_02 | Semantic inference required |
| 12 | `correct_output/capacity_eviction` | 19 | task_06 | Policy inference required |
| 13 | `correct_output/ownership_override` | 18 | task_08 | Access control semantics |
| 14 | `correct_output/dependency_order` | 15 | task_09 | Scheduling logic |
| 15 | `correct_output/basic` | 14 | various | Regression — models break earlier phases |
| 16 | `correct_output/basic_arithmetic` | 11 | task_07 | Expression parsing basics |
| 17 | `correct_output/multi_key` | 10 | task_04 | Multi-sort convention |
| 18 | `correct_output/basic_whitespace` | 9 | task_05 | Phase 0 failures |
| 19 | `correct_output/list_merge` | 9 | task_02 | Deduplication semantics |
| 20 | `correct_output/range_matching` | 8 | task_08 | Access control pattern matching |

### 5.2 Violation Patterns

**Actionable feedback** (violation name hints at solution): `divisible_by_7`, `basic_arithmetic`, `basic_whitespace` — models that read the violation name can infer the fix. 5/14 models succeed on `divisible_by_7`.

**Semantic inference required** (violation names what's wrong but not how to fix): `negative_handling`, `numeric_conflict`, `string_conflict` — 0% success rate. The gap between "knowing the test name" and "inferring the expected behavior" is too large.

**Domain knowledge required** (violation points to a concept the model may not know): `unicode_combining`, `ttl_expiry` — these test whether the model has prior knowledge of Unicode NFC or cache TTL patterns.

**Invariant violations** (`no_mutation/error` — 43x): Models mutate input data. This is an SAOTRI Invariant (I) — a safety constraint that persists throughout. High violation count suggests models don't consistently protect inputs.

---

## 6. Infrastructure Issues

### 6.1 Network Errors from Parallel Execution

Running 14 models simultaneously against OpenRouter causes connection drops:

| Issue | Count | Impact |
|-------|-------|--------|
| `RemoteProtocolError: Server disconnected` | 8+ instances | Premature run termination |
| Timeout with 0 tokens (API never responded) | 2 instances | Wasted test slot |
| Empty response requiring retry | 1+ instances | Delayed execution |

**Affected models** (disproportionately slow providers): Gemini 3.1 Pro (5 errors), Kimi K2.5 (612s timeout),  (3 errors).

**Recommendation**: Re-run impacted models at `--parallel 5` for reliable results.

### 6.2 Duration Extremes

| Duration | Model | Task | Status | Tokens | Issue |
|----------|-------|------|--------|--------|-------|
| 1999s | Kimi K2.5 | task_04_sort_objects | failed | 49K | Slow API + long refinement loop |
| 1723s | GPT-5.2 Codex | task_04_sort_objects | failed | 71K | Excessive refinement attempts |
| 1239s | Kimi K2.5 | task_01_transform_list | error | 33K | API disconnect after long run |
| 1182s | Gemini 3.1 Pro | task_05_text_processor | failed | 69K | Slow generation |
| 926s | Grok 4.1 Fast | task_01_transform_list | failed | 100K | Token-heavy with no progress |

### 6.3 Token Waste

Top consumers that fail:

| Tokens | Model | Task | Status | Insight |
|--------|-------|------|--------|---------|
| 100,443 | Grok 4.1 Fast | task_01_transform_list | failed | 100K tokens, 2/3 phases — brute force |
| 92,191 | Grok 4.1 Fast | task_04_sort_objects | failed | Generates long code without learning |
| 91,290 | DeepSeek V3.2 | task_07_expression_parser | error | 6/9 phases then API crash |
| 80,892 | Grok 4.1 Fast | task_05_text_processor | failed | Pattern: high tokens, low progress |
| 80,852 | Grok 4.1 Fast | task_00_fizzbuzz | failed | Even on easy tasks |

**Benchmark insight**: Grok 4.1 Fast consistently consumes 5-10x more tokens than successful models on the same task, confirming SaotriBench's thesis that "token efficiency indicates comprehension."

---

## 7. Alignment with SaotriBench Design Goals

### 7.1 SAOTRI Framework Validation

| Component | What It Tests | Evidence from This Run |
|-----------|--------------|----------------------|
| **State (S)** — Hidden constraints | Agents cannot see test logic | Confirmed: no model has access to test source |
| **Actions (A)** — Code submissions | Agents generate solution.py | Working correctly |
| **Observations (O)** — Structured feedback | Violation names + coverage % | Working, but signal strength varies by task (see 7.2) |
| **Transitions (T)** — Phase invalidation | New phases break old solutions | Confirmed: coverage drops on phase advance (e.g., 100% → 78.6% on fizzbuzz phase 1) |
| **Resilience (R)** — Accumulating requirements | Must retain all prior rules | Violation `correct_output/basic` (14x) shows some models regress on earlier phases |
| **Invariants (I)** — Safety constraints | `no_mutation` must hold always | 43 violations — models frequently break invariants |

### 7.2 Feedback Signal Effectiveness

The core question: **Is structured violation feedback sufficient for agents to discover hidden requirements?**

| Feedback Quality | Tasks | Evidence |
|-----------------|-------|---------|
| **Effective** | task_03 (92%), task_00 (36%) | Violation names map to solutions; multiple models learn |
| **Partially effective** | task_04 (7%) | Only the strongest model succeeds; others see violations but can't infer conventions |
| **Potentially insufficient** | task_01 (0%), task_02 (0%), task_05 (0%), task_06 (0%) | Zero models succeed despite receiving violation feedback |

**Key question**: When 0/14 models (including the strongest available) fail a task labeled "easy", is this measuring hidden-requirement discovery capability, or is the feedback signal too weak for the task's hidden requirements?

The contrast with task_03 (92%) and task_00 (36%) suggests the issue is task-specific, not framework-wide. The benchmark infrastructure works — some tasks may need feedback enrichment.

### 7.3 Difficulty Label Accuracy

| Label | Tasks | Avg Completion | Assessment |
|-------|-------|---------------|------------|
| easy | task_00, task_01, task_02 | 12% (5/38) | **Mislabeled** — only 1 of 3 easy tasks is achievable by most models |
| medium | task_03 through task_08 | 17% (13/78) | Accurate — moderate completion driven by task_03 |
| hard | task_09, task_10, task_11 | 0% (0/13) | Cannot assess — insufficient coverage |

**Recommendation**: Consider relabeling task_01 and task_02 to "medium" or enriching their feedback signals to match their "easy" label.

---

## 8. Recommendations

### 8.1 Immediate (before publishing results)

| Priority | Action | Rationale |
|----------|--------|-----------|
| P0 | Re-run error/timeout results at `--parallel 5` | 31 results (27%) are infrastructure failures, not model failures |
| P0 | Complete coverage for tasks 07-11 | Only 3-5 models tested; insufficient for comparison |
| P1 | Investigate task_01/task_02 feedback signals | 0% completion on "easy" tasks may indicate insufficient feedback rather than model weakness |

### 8.2 Feedback Signal Improvements

| Task | Current Feedback | Suggested Enhancement |
|------|-----------------|----------------------|
| task_01 | `negative_handling: FAIL` | Include expected output sample: `input=[-3] → expected=[6]` |
| task_02 | `numeric_conflict: FAIL` | Include conflict example: `a={x:10}, b={x:5} → expected={x:15}` |
| task_06 | `ttl_expiry: FAIL` | Include timing info: `key="x" expired at t=5, accessed at t=6` |

This preserves the hidden-requirement philosophy while giving agents actionable feedback — they still must infer the general rule from examples.

### 8.3 Infrastructure

| Priority | Action |
|----------|--------|
| P1 | Add exponential backoff + retry in `llm_client.py` to handle OpenRouter drops |
| P1 | Separate `error` (infrastructure) from `failed` (model) in status reporting |
| P2 | Default `--parallel` to 5 instead of allowing unlimited |

### 8.4 Analysis (after full run completes)

| Action | Purpose |
|--------|---------|
| Re-assess difficulty labels based on full-coverage completion rates | Ensure "easy" tasks are actually achievable |
| Compute token-efficiency scores (tokens per completed phase) | Quantify the "comprehension vs brute force" insight |
| Identify whether any model completes task_01/02/05/06 with full coverage | Determine if these are fundamentally too hard or just need more models |

---

## 9. Appendix: Full Completion Heatmap

```
Model                   fizz  list  dict  brkt  sort  text  cach  expr  ctrl  sched pipe  vers
                        (4ph) (3ph) (4ph) (5ph) (6ph) (7ph) (8ph) (9ph) (10p) (12p) (12p) (15p)
─────────────────────── ───── ───── ───── ───── ───── ───── ───── ───── ───── ───── ───── ─────
Claude Opus 4.6         4/4   2/3   3/4   5/5   6/6   1/7   2/8    --    --    --    --    --
GPT-5.2 Codex           4/4   2/3   1/4   5/5   3/6   1/7   2/8    --    --    --    --    --
Claude Sonnet 4.6       4/4   2/3   3/4   5/5   4/6   1/7   2/8   0/9   2/10  1/12   --   5/15
DeepSeek V3.2           4/4   1/3   1/4   5/5   2/6   2/7   2/8   6/9   2/10  2/12   --   1/15
Gemini 3.1 Pro          4/4   2/3   3/4   5/5   2/6   2/7   2/8   0/9   2/10  1/12  2/12  1/15
GLM 5                   1/4   2/3    --   5/5   2/6   2/7   2/8    --    --    --    --    --
MiniMax M2.5            1/4   1/3    --   5/5   3/6   2/7   2/8    --    --    --    --    --
            1/4   2/3   1/4   5/5   3/6   2/7   2/8    --    --    --    --    --
Grok 4.1 Fast           1/4   2/3   0/4   5/5   4/6   2/7   2/8    --    --    --    --    --
Kimi K2.5               1/4   1/3    --   5/5   3/6   2/7    --    --    --    --    --    --
           1/4   1/3   1/4   5/5   3/6   1/7   1/8    --    --    --    --    --
Llama 3.3 70B           1/4   1/3   1/4   5/5   2/6   2/7   2/8   0/9   2/10  1/12  2/12  1/15
Trinity Large           1/4   1/3    --    --   2/6   1/7   2/8    --    --    --    --    --
              1/4   1/3   1/4   3/5   2/6   0/7   1/8   0/9   1/10  1/12  0/12  1/15

Legend: phases_completed/total_phases   -- = not yet tested
        Bold completions: 4/4, 5/5, 6/6 = task fully completed
```

---

*This audit will be updated after the benchmark run completes and all 168 model+task pairs have results.*
