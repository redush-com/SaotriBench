# Technical Audit of SaotriBench Project
**Date:** February 21, 2026

## 1. Stated Benchmark Goals

SaotriBench is positioned as a benchmark for evaluating LLM agents on multi-phase programming tasks. The primary research goal is to determine whether agents are capable of **building an internal model of projects**, rather than simply relying on pattern matching. 

Three key capabilities are tested:
1. **Hidden requirement discovery** — inferring undisclosed constraints from structured feedback (test execution errors).
2. **Long-context retention** — maintaining context (state, hypotheses) across multiple iterations.
3. **Iterative refinement** — systematically improving solutions based on violation signals.

## 2. Key Parameters for Evaluation (Audit Metrics)

The following parameters were selected for the technical audit:
- **Signal Validity** — the extent to which the results reflect the claimed skills rather than external factors.
- **Infrastructure Constraints (Experiment Purity)** — the absence of technical bottlenecks that distort the results.
- **Feedback Design (Informativeness and Transparency)** — the balance between direct hints and complete uncertainty for the LLM.
- **Reproducibility** — the consistency of the testing results.

## 3. Evaluation of Implementation vs. Stated Goals

### ✅ What Aligns with the Goals (Strengths)
* **Testing Iterative Refinement:** The benchmark design with phase accumulation, where a Phase N solution must satisfy all rules of phases 0..N-1, excellently identifies models prone to regressions. This works successfully as a stress test.
* **Feedback Format:** Providing the agent with structured JSON feedback (indicating `rule_id`, `scope`, and coverage) without revealing the actual test cases is a great implementation of the hidden requirement discovery mechanic. 
* **Task Architecture:** The difficulty gradation (Easy/Medium/Hard) works correctly. Models consistently pass basic tasks but demonstrate a clear separation of capabilities on Medium and Hard.

### ❌ What Does NOT Align with the Goals and to What Extent (Weaknesses)
* **Solution Leakage via Scope Names (Validity Issue):** Names like `"divisible_by_7"`, `"right_associativity"` or `"peer_constraint"` directly tell the model what needs to be done. Instead of testing the ability to *build an internal model* or make *inferences* based on failure patterns, this turns into a reading comprehension test. Because of this, 50-70% of the benchmark does not test what it claims to.
* **Conflating Algorithm Testing with Logical Inference Testing:** At the Hard level (e.g., *Version Resolver Phase 2: transitive dependencies* or *Schedule Optimizer Phase 2*), the tests require the model to know specific complex algorithms (graphs, topological sorting). Models fail these not because they cannot discover a hidden requirement, but because they lack specialized algorithmic knowledge. Furthermore, tasks like `task_11_version_resolver` explicitly test performance constraints (e.g., Phase 14 requires handling large registries within 5.0s limit), conflating optimization skills with inference skills.
* **Failure of the Long-Context Retention Test:** The benchmark collapses under its own weight. Due to the strict `max_tokens=4096` limit, most models experience context overflow during a long history of attempts, resulting in an `EmptyResponseError`. Consequently, the test measures *hard systemic limits of the API infrastructure* rather than the ability to *retain* context.
* **Non-determinism (Weak Reproducibility):** The evaluation is based on a single run, but LLM models (even with `temperature=0.1`) show a 10% to 20% variance in results between runs.

## 4. Critically Necessary Changes

In order for the benchmark to genuinely measure the ability of LLM agents to build an "internal model" and to operate stably, the following changes are **critically necessary**:

### Architecture and Infrastructure
1. **Increase `max_tokens` to 8192+:** The current limit of 4096 tokens artificially kills runs (especially for GPT-5.2, Kimi, GLM). This is the *most urgent technical issue*. Alternatively: implement a conversation history compression mechanism before sending it to the agent.
2. **Multiple Runs (Best-of-N):** Introduce a mechanism to run a model 3 times per task and select the median result to smooth out the non-determinism of the LLM APIs.
3. **Error Logging & Handling:** JSON reports (`reports/*.json`) must save full traces of technical errors (syntax errors, import violations, timeouts), which are currently only printed to the console. Furthermore, the `_extract_code` method in the LLM client defaults to returning the full response text if no code blocks are found, which simply pushes the error down to the syntax evaluation stage, masking the true root cause (formatting failure).

### Task Design
4. **Scope Name Obfuscation:** For Medium and Hard tasks, rename the `scope` in the feedback. Instead of `"divisible_by_7"`, use opaque codes (e.g., `"violation_A3"` or hashes). This will force agents to make genuine inferences and build hypotheses based on changes in test coverage, rather than parsing explicit hints.
5. **Add Regression-Only Phases:** Introduce phases that do not add new rules but introduce new complex edge cases to existing rules. This will test the robustness of the generated code without changing the requirements.
6. **Separate Algorithms and Inference:** For Hard tier tasks, provide agents with base algorithm templates (e.g., a skeleton for graph traversal) in `problem.md` so that the benchmark specifically tests *building a requirement model* rather than *recalling an algorithm from the training data*. Tests focusing solely on performance parameters (like `time.perf_counter()` limits in `task_11_version_resolver`) should ideally be separated into a distinct evaluation dimension.

## Summary
The current implementation of SaotriBench is an excellent tool for testing **iterative programming skills** (B+ rating). However, to achieve its primary research goal (evaluating the "internal model"), the benchmark requires urgent recalibration: eliminating solution leaks via `scope` and lifting the artificial infrastructure ceiling on tokens.