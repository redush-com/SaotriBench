# Technical Audit of SaotriBench Project
**Date:** February 21, 2026

## 1. Stated Benchmark Goals

SaotriBench is positioned as a benchmark for evaluating LLM agents on multi-phase programming tasks. The primary research goal is to determine whether agents are capable of **building an internal model of projects**, rather than simply relying on pattern matching. 

Three key capabilities are tested:
1. **Hidden requirement discovery** — inferring undisclosed constraints from structured feedback (test execution errors).
2. **Long-context retention** — maintaining context (state, hypotheses) across multiple iterations.
3. **Iterative refinement** — systematically improving solutions based on violation signals.

## 2. Key Parameters for Evaluation (Audit Metrics)

The following parameters were selected for the technical audit:
- **Signal Validity** — the extent to which the results reflect the claimed skills rather than external factors.
- **Infrastructure Constraints (Experiment Purity)** — the absence of technical bottlenecks that distort the results.
- **Feedback Design (Informativeness and Transparency)** — the balance between direct hints and complete uncertainty for the LLM.
- **Reproducibility** — the consistency of the testing results.

## 3. Evaluation of Implementation vs. Stated Goals

### ✅ What Aligns with the Goals (Strengths)
* **Testing Iterative Refinement:** The benchmark design with phase accumulation, where a Phase N solution must satisfy all rules of phases 0..N-1, excellently identifies models prone to regressions. This works successfully as a stress test.
* **Feedback Format:** Providing the agent with structured JSON feedback (indicating `rule_id`, `scope`, and coverage) without revealing the actual test cases is a great implementation of the hidden requirement discovery mechanic. 
* **Task Architecture:** The difficulty gradation (Easy/Medium/Hard) works correctly. Models consistently pass basic tasks but demonstrate a clear separation of capabilities on Medium and Hard.

### ❌ What Does NOT Align with the Goals and to What Extent (Weaknesses)
* **Solution Leakage via Scope Names (Validity Issue):** Names like `"divisible_by_7"`, `"right_associativity"` or `"peer_constraint"` directly tell the model what needs to be done. Instead of testing the ability to *build an internal model* or make *inferences* based on failure patterns, this turns into a reading comprehension test. Because of this, 50-70% of the benchmark does not test what it claims to.
* **Conflating Algorithm Testing with Logical Inference Testing:** At the Hard level (e.g., *Version Resolver Phase 2: transitive dependencies* or *Schedule Optimizer Phase 2*), the tests require the model to know specific complex algorithms (graphs, topological sorting). Models fail these not because they cannot discover a hidden requirement, but because they lack specialized algorithmic knowledge. Furthermore, tasks like `task_11_version_resolver` explicitly test performance constraints (e.g., Phase 14 requires handling large registries within 5.0s limit), conflating optimization skills with inference skills.
* **Failure of the Long-Context Retention Test:** The benchmark collapses under its own weight. Due to the strict `max_tokens=4096` limit, most models experience context overflow during a long history of attempts, resulting in an `EmptyResponseError`. Consequently, the test measures *hard systemic limits of the API infrastructure* rather than the ability to *retain* context.
* **Non-determinism (Weak Reproducibility):** The evaluation is based on a single run, but LLM models (even with `temperature=0.1`) show a 10% to 20% variance in results between runs.

## 4. Critically Necessary Changes

In order for the benchmark to genuinely measure the ability of LLM agents to build an "internal model" and to operate stably, the following changes are **critically necessary**:

### Architecture and Infrastructure
1. **[✓ IMPLEMENTED] Increase `max_tokens` to 8192+:** The default limit in `agents/config.py` was updated to 8192 to prevent artificial token exhaustion on complex tasks.
2. **[ ] Multiple Runs (Best-of-N):** Introduce a mechanism to run a model 3 times per task and select the median result to smooth out the non-determinism of the LLM APIs.
3. **[ ] Error Logging & Handling:** JSON reports (`reports/*.json`) must save full traces of technical errors (syntax errors, import violations, timeouts), which are currently only printed to the console. Furthermore, the `_extract_code` method in the LLM client defaults to returning the full response text if no code blocks are found, which simply pushes the error down to the syntax evaluation stage, masking the true root cause (formatting failure).

### Task Design
4. **[✓ IMPLEMENTED] Scope Name Obfuscation:** The `Runner` (`saotri_bench/runner.py`) now obfuscates scope names into deterministic hashes (e.g., `scope_a1b2c3`) before presenting them to the LLM agent, forcing genuine inference rather than parsing explicit hints.
5. **[✓ IMPLEMENTED] Add Regression-Only Phases:** Regression phases (testing new edge cases without changing rules) were added to tasks like `task_00_fizzbuzz` and `task_03_validate_brackets` to verify robustness.
6. **[✓ IMPLEMENTED] Separate Algorithms and Inference:** Hard tier tasks (`task_09`, `task_10`, `task_11`) now provide agents with base algorithm templates in `problem.md` to focus the test on requirement modeling. Furthermore, tests focusing solely on performance parameters were cleanly separated into distinct `Expert` evaluation tasks (e.g., `task_11_version_resolver_perf`).

## Summary
The current implementation of SaotriBench is an excellent tool for testing **iterative programming skills** (B+ rating). However, to achieve its primary research goal (evaluating the "internal model"), the benchmark requires urgent recalibration: eliminating solution leaks via `scope` and lifting the artificial infrastructure ceiling on tokens.

---

### Implementation Status Update (As of Feb 21, 2026)
Significant progress has been made to address the critical flaws:
- The **Scope Leakage** issue is fully resolved via runtime string hashing in the feedback loop. 
- **Conflated testing goals** are resolved by splitting pure performance optimization into separate `_perf` tasks and by providing algorithm templates for pure-inference logic tasks.
- **Context retention issues** are mitigated by doubling the token limit in the model configurations.
- Two remaining infrastructural improvements (Best-of-N running and deeper error logging) remain on the roadmap for a future release.